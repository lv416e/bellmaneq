{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration and the Bellman Equation\n",
    "\n",
    "This notebook demonstrates the core Bellman equation solver on a simple GridWorld MDP.\n",
    "We compare **Value Iteration**, **Policy Iteration**, and **Modified Policy Iteration**,\n",
    "visualizing their convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bellmaneq\n",
    "from bellmaneq.viz import plot_convergence, plot_solver_comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a GridWorld MDP\n",
    "\n",
    "We define a 4x4 GridWorld with:\n",
    "- 16 states (positions on the grid)\n",
    "- 4 actions (up, down, left, right)\n",
    "- State 15 (bottom-right) is the goal with reward +1\n",
    "- All other transitions yield reward -0.04 (step penalty)\n",
    "- Movements that would leave the grid keep the agent in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 4\n",
    "N_STATES = GRID_SIZE * GRID_SIZE\n",
    "N_ACTIONS = 4  # up, down, left, right\n",
    "GOAL = N_STATES - 1\n",
    "STEP_PENALTY = -0.04\n",
    "GOAL_REWARD = 1.0\n",
    "\n",
    "# Action deltas: up=(-1,0), down=(+1,0), left=(0,-1), right=(0,+1)\n",
    "DELTAS = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "def to_state(row, col):\n",
    "    return row * GRID_SIZE + col\n",
    "\n",
    "def from_state(s):\n",
    "    return divmod(s, GRID_SIZE)\n",
    "\n",
    "# Build reward and transition matrices\n",
    "rewards = np.full((N_STATES, N_ACTIONS), STEP_PENALTY)\n",
    "transitions = np.zeros(N_STATES * N_ACTIONS * N_STATES)\n",
    "\n",
    "for s in range(N_STATES):\n",
    "    row, col = from_state(s)\n",
    "    for a, (dr, dc) in enumerate(DELTAS):\n",
    "        nr, nc = row + dr, col + dc\n",
    "        if 0 <= nr < GRID_SIZE and 0 <= nc < GRID_SIZE:\n",
    "            ns = to_state(nr, nc)\n",
    "        else:\n",
    "            ns = s  # stay in place\n",
    "        transitions[s * N_ACTIONS * N_STATES + a * N_STATES + ns] = 1.0\n",
    "        if ns == GOAL:\n",
    "            rewards[s, a] = GOAL_REWARD\n",
    "\n",
    "print(f'MDP: {N_STATES} states, {N_ACTIONS} actions')\n",
    "print(f'Reward matrix shape: {rewards.shape}')\n",
    "print(f'Transition tensor elements: {len(transitions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with Value Iteration\n",
    "\n",
    "The Bellman optimality equation:\n",
    "\n",
    "$$V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]$$\n",
    "\n",
    "Value Iteration repeatedly applies the Bellman operator $T$:\n",
    "$$V_{k+1} = TV_k$$\n",
    "\n",
    "By the Banach fixed-point theorem, this converges geometrically at rate $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "\n",
    "vi_result = bellmaneq.solve_value_iteration(rewards, transitions, gamma=gamma, tol=1e-10)\n",
    "\n",
    "print(f'Converged: {vi_result.converged}')\n",
    "print(f'Iterations: {vi_result.iterations}')\n",
    "\n",
    "# Reshape values into the grid\n",
    "values = vi_result.get_values().reshape(GRID_SIZE, GRID_SIZE)\n",
    "policy = vi_result.get_policy().reshape(GRID_SIZE, GRID_SIZE)\n",
    "\n",
    "print('\\nOptimal Value Function:')\n",
    "print(np.round(values, 2))\n",
    "\n",
    "ACTION_SYMBOLS = ['\\u2191', '\\u2193', '\\u2190', '\\u2192']  # up, down, left, right\n",
    "print('\\nOptimal Policy:')\n",
    "for r in range(GRID_SIZE):\n",
    "    row_str = ''\n",
    "    for c in range(GRID_SIZE):\n",
    "        if to_state(r, c) == GOAL:\n",
    "            row_str += ' G '\n",
    "        else:\n",
    "            row_str += f' {ACTION_SYMBOLS[policy[r, c]]} '\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Value function heatmap\n",
    "im = axes[0].imshow(values, cmap='YlOrRd', interpolation='nearest')\n",
    "axes[0].set_title('Optimal Value Function V*(s)', fontsize=13)\n",
    "for r in range(GRID_SIZE):\n",
    "    for c in range(GRID_SIZE):\n",
    "        axes[0].text(c, r, f'{values[r, c]:.2f}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Policy arrows\n",
    "axes[1].set_xlim(-0.5, GRID_SIZE - 0.5)\n",
    "axes[1].set_ylim(GRID_SIZE - 0.5, -0.5)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].set_title('Optimal Policy \\u03C0*(s)', fontsize=13)\n",
    "ARROW_DX = [0, 0, -0.3, 0.3]\n",
    "ARROW_DY = [-0.3, 0.3, 0, 0]\n",
    "for r in range(GRID_SIZE):\n",
    "    for c in range(GRID_SIZE):\n",
    "        if to_state(r, c) == GOAL:\n",
    "            axes[1].plot(c, r, 'g*', markersize=20)\n",
    "        else:\n",
    "            a = int(policy[r, c])\n",
    "            axes[1].annotate(\n",
    "                '', \n",
    "                xy=(c + ARROW_DX[a], r + ARROW_DY[a]),\n",
    "                xytext=(c, r),\n",
    "                arrowprops=dict(arrowstyle='->', color='navy', lw=2)\n",
    "            )\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis: VI vs PI\n",
    "\n",
    "Policy Iteration converges in far fewer iterations (typically in single digits),\n",
    "while Value Iteration requires many more but each iteration is cheaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_result = bellmaneq.solve_policy_iteration(rewards, transitions, gamma=gamma)\n",
    "\n",
    "print(f'Value Iteration:  {vi_result.iterations} iterations')\n",
    "print(f'Policy Iteration: {pi_result.iterations} iterations')\n",
    "\n",
    "# Verify both solvers agree\n",
    "vi_vals = vi_result.get_values()\n",
    "pi_vals = pi_result.get_values()\n",
    "max_diff = np.max(np.abs(vi_vals - pi_vals))\n",
    "print(f'\\nMax |V_vi - V_pi|: {max_diff:.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence curves\n",
    "histories = {\n",
    "    'Value Iteration': vi_result.get_convergence_history(),\n",
    "    'Policy Iteration': pi_result.get_convergence_history(),\n",
    "}\n",
    "fig = plot_solver_comparison(histories, title='Convergence: VI vs PI on GridWorld')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Factor Sensitivity\n",
    "\n",
    "The discount factor $\\gamma$ controls the planning horizon. Higher $\\gamma$ means\n",
    "the agent looks further into the future, but convergence slows down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.5, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(gammas), figsize=(4 * len(gammas), 4))\n",
    "\n",
    "for ax, g in zip(axes, gammas):\n",
    "    result = bellmaneq.solve_value_iteration(rewards, transitions, gamma=g)\n",
    "    v = result.get_values().reshape(GRID_SIZE, GRID_SIZE)\n",
    "    ax.imshow(v, cmap='YlOrRd', interpolation='nearest')\n",
    "    ax.set_title(f'\\u03B3 = {g}\\n({result.iterations} iter)', fontsize=11)\n",
    "    for r in range(GRID_SIZE):\n",
    "        for c in range(GRID_SIZE):\n",
    "            ax.text(c, r, f'{v[r, c]:.1f}', ha='center', va='center', fontsize=8)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Effect of Discount Factor on the Value Function', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bellmaneq (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
